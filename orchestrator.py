# -*- coding: utf-8 -*-
"""orchestrator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K_oAvCKmSgMWmihWrk1airUd3tY7tESd
"""

import yaml
import logging

from memory import ConversationMemory, history_to_json
from conversation_agent import OpenAIConversationAgent
from user_clustering_agent import UserClusteringAgent
from shards_retrieval import ShardedRetrievalAgent
from rerank import TwoStageContextualRerankerJSON
from utils import api_key
from embeddings import setup_embeddings_cpu

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")


class RecommenderOrchestrator:
    def __init__(self, config_path: str):
        # Load configuration from YAML
        with open(config_path, "r") as f:
            self.config = yaml.safe_load(f)

        # Shared resources
        self.memory     = ConversationMemory()
        self.embeddings = setup_embeddings_cpu()

        # Full‐module agents (files are in repo root)
        # conversation_agent.py          ← https://huggingface.co/spaces/agurusantosh/AI_Food_Recommender/resolve/main/conversation_agent.py
        # memory.py                      ← https://huggingface.co/spaces/agurusantosh/AI_Food_Recommender/resolve/main/memory.py
        # rerank.py                      ← https://huggingface.co/spaces/agurusantosh/AI_Food_Recommender/resolve/main/rerank.py
        # shards_retrieval.py            ← https://huggingface.co/spaces/agurusantosh/AI_Food_Recommender/resolve/main/shards_retrieval.py
        # user_clustering_agent.py       ← https://huggingface.co/spaces/agurusantosh/AI_Food_Recommender/resolve/main/user_clustering_agent.py
        # utils.py                       ← https://huggingface.co/spaces/agurusantosh/AI_Food_Recommender/resolve/main/utils.py

        self.cluster_agent     = UserClusteringAgent(model_path=self.config["cluster_model_path"])
        self.conv_agent        = OpenAIConversationAgent(
                                    memory=self.memory,
                                    embeddings=self.embeddings
                                )
        self.retrieval_agent   = ShardedRetrievalAgent(
                                    shard_info_path=self.config["shard_info_path"],
                                    embeddings=self.embeddings
                                )
        self.reranker_agent    = TwoStageContextualRerankerJSON(
                                    model=self.config["rerank_model"],
                                    api_key=api_key
                                )

    def initialize_user(self,
                        age: float,
                        income: float,
                        gender: int,
                        weather: int,
                        marital_status: int):
        """
        Run clustering on profile form submission and store cluster in memory.
        """
        cluster_id = self.cluster_agent.predict_cluster(
            Age=age,
            Monthly_Income=income,
            Gender=gender,
            weather=weather,
            Marital_Status=marital_status
        )
        self.memory.add_metadata("user_cluster", cluster_id)
        logger.info(f"Assigned user to cluster {cluster_id}")

    def handle_chat(self,
                    user_message: str,
                    recommendations_context: dict = None) -> dict:
        """
        Handle a single chat turn: update slots/intent, optionally run retrieval & rerank,
        then return the assembled response dict.
        """
        # 1. Delegate to conversation agent
        conv_response = self.conv_agent.handle_turn(user_message, recommendations_context)

        # 2. If ready, run retrieval + rerank pipeline
        if conv_response["action"] in ("SEARCH", "SEARCH_READY"):
            query, filt = conv_response["query"], conv_response["filter"]
            docs = self.retrieval_agent.get_all_docs_formatted(
                query,
                filt,
                top_k_per_shard=self.config["top_k_per_shard"]
            )
            history_json = history_to_json(self.memory.history)
            rerank_res = self.reranker_agent.rerank_with_context(docs, history_json)

            conv_response["recommendations"]    = rerank_res.get("top_10_documents", [])
            conv_response["ranking_conditions"] = rerank_res.get("ranking_conditions", [])

        # 3. Record turn in memory
        self.memory.add_turn(user_message, conv_response["response"])
        return conv_response
